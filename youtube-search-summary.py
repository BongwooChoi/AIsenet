import streamlit as st
import google.generativeai as genai
from googleapiclient.discovery import build
from youtube_transcript_api import YouTubeTranscriptApi
import os
import time
from datetime import datetime, timedelta
import requests
import yfinance as yf
import threading

# API í‚¤ ì„¤ì •
genai.configure(api_key=st.secrets["GOOGLE_AI_STUDIO_API_KEY"])
youtube = build('youtube', 'v3', developerKey=st.secrets["YOUTUBE_API_KEY"])

# ì§€ìˆ˜ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_latest_index():
    # Yahoo Financeì—ì„œ ì§€ìˆ˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    sp500 = yf.Ticker('^GSPC').history(period='1d')['Close'][0]
    nasdaq = yf.Ticker('^IXIC').history(period='1d')['Close'][0]
    dowjones = yf.Ticker('^DJI').history(period='1d')['Close'][0]
    kospi = yf.Ticker('^KS11').history(period='1d')['Close'][0]
    kosdaq = yf.Ticker('^KQ11').history(period='1d')['Close'][0]
    
    index_info = {
        'S&P 500': f"{sp500:.2f}",
        'Nasdaq': f"{nasdaq:.2f}",
        'Dow Jones': f"{dowjones:.2f}",
        'KOSPI': f"{kospi:.2f}",
        'KOSDAQ': f"{kosdaq:.2f}"
    }
    return index_info

# ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ (Serp API ì‚¬ìš©)
def search_news(query, published_after, max_results=10):
    api_key = st.secrets["SERP_API_KEY"]
    url = f"https://serpapi.com/search.json?q={query}&tbm=nws&api_key={api_key}&num={max_results}&sort=date"
    
    response = requests.get(url)
    news_data = response.json()
    articles = news_data.get('news_results', [])
    
    # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
    unique_articles = []
    seen_urls = set()
    for article in articles:
        if article['link'] not in seen_urls:
            unique_articles.append({
                'title': article.get('title', ''),
                'source': {'name': article.get('source', '')},
                'description': article.get('snippet', ''),
                'url': article.get('link', ''),
                'content': article.get('snippet', '')  # Serp APIì—ëŠ” contentê°€ ì—†ìœ¼ë¯€ë¡œ snippetìœ¼ë¡œ ëŒ€ì²´
            })
            seen_urls.add(article['link'])
        if len(unique_articles) == max_results:
            break
    
    return unique_articles

# ìœ íŠœë¸Œ ê²€ìƒ‰ ë° ìµœì‹  ìˆœ ì •ë ¬ í•¨ìˆ˜
def search_videos_with_transcript(query, published_after, max_results=10):
    request = youtube.search().list(
        q=query,
        type='video',
        part='id,snippet',
        order='relevance',
        publishedAfter=published_after,
        maxResults=max_results
    )
    response = request.execute()

    videos_with_transcript = []
    for item in response['items']:
        video_id = item['id']['videoId']
        if get_video_transcript(video_id):
            videos_with_transcript.append(item)
    
    videos_with_transcript.sort(key=lambda x: x['snippet']['publishedAt'], reverse=True)
    
    return videos_with_transcript[:max_results], len(response['items'])


# ì¡°íšŒ ê¸°ê°„ ì„ íƒ í•¨ìˆ˜
def get_published_after(option):
    today = datetime.utcnow()
    if option == "ìµœê·¼ 1ì¼":
        return (today - timedelta(days=1)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 1ì£¼ì¼":
        return (today - timedelta(weeks=1)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 1ê°œì›”":
        return (today - timedelta(weeks=4)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 3ê°œì›”":
        return (today - timedelta(weeks=12)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 6ê°œì›”":
        return (today - timedelta(weeks=24)).isoformat("T") + "Z"
    elif option == "ìµœê·¼ 1ë…„":
        return (today - timedelta(weeks=52)).isoformat("T") + "Z"
    else:
        return None  # ì´ ê²½ìš° ì¡°íšŒ ê¸°ê°„ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ

# ìë§‰ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ (YouTube Transcript API ì‚¬ìš©)
def get_video_transcript(video_id):
    try:
        transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['ko', 'en'])
        return ' '.join([entry['text'] for entry in transcript])
    except Exception as e:
        return None

# YouTube ì˜ìƒ ìš”ì•½ í•¨ìˆ˜
def summarize_video(video_id, video_title):
    try:
        transcript = get_video_transcript(video_id)
        if not transcript:
            return "ìë§‰ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        model = genai.GenerativeModel('gemini-1.5-pro')
        prompt = f"ë‹¤ìŒ YouTube ì˜ìƒì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ê°€ë…ì„± ìˆëŠ” í•œ í˜ì´ì§€ì˜ ë³´ê³ ì„œ í˜•íƒœë¡œ ìš”ì•½í•˜ì„¸ìš”. ìµœì¢… ê²°ê³¼ëŠ” í•œêµ­ì–´ë¡œ ë‚˜ì™€ì•¼ í•©ë‹ˆë‹¤.:\n\nì œëª©: {video_title}\n\n{transcript}"
        response = model.generate_content(prompt)

        if not response or not response.parts:
            feedback = response.prompt_feedback if response else "No response received."
            return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {feedback}"

        summary = response.text
        return summary
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ í•¨ìˆ˜
def summarize_news_article(article):
    try:
        model = genai.GenerativeModel('gemini-1.5-pro')
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ë‚´ìš©ì„ ê°€ë…ì„± ìˆëŠ” í•œ í˜ì´ì§€ì˜ ë³´ê³ ì„œ í˜•íƒœë¡œ ìš”ì•½í•˜ì„¸ìš”. 
ì›ë¬¸ì´ ì˜ì–´ì¸ ê²½ìš°ì—ë„ ìµœì¢… ê²°ê³¼ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ë˜í•œ, ìš”ì•½ ë§ˆì§€ë§‰ì— ì›ë¬¸ì˜ ì–¸ì–´(í•œêµ­ì–´ ë˜ëŠ” ì˜ì–´)ë¥¼ ëª…ì‹œí•´ ì£¼ì„¸ìš”.

ì œëª©: {article['title']}

ë‚´ìš©: {article['content']}
"""
        response = model.generate_content(prompt)

        if not response or not response.parts:
            feedback = response.prompt_feedback if response else "No response received."
            return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {feedback}"

        summary = response.text
        return summary
    except Exception as e:
        return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
def download_summary_file(summary_text, file_name="summary.txt"):
    st.download_button(
        label="ë‹¤ìš´ë¡œë“œ",
        data=summary_text,
        file_name=file_name,
        mime="text/plain"
    )

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="AI YouTube & ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìš”ì•½", page_icon="ğŸ“°", layout="wide")

# Streamlit ì•±
st.title("ğŸ“° AI YouTube & ë‰´ìŠ¤ ê²€ìƒ‰ ë° ìš”ì•½ ì„œë¹„ìŠ¤")
st.markdown("ì´ ì„œë¹„ìŠ¤ëŠ” YouTube ì˜ìƒê³¼ ë‰´ìŠ¤(í•œêµ­ì–´ ë° ì˜ì–´)ë¥¼ ê²€ìƒ‰í•˜ê³  AIë¥¼ ì´ìš©í•´ ìš”ì•½ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì— ê²€ìƒ‰ ì¡°ê±´ì„ ì…ë ¥í•˜ê³  ê²€ìƒ‰í•´ë³´ì„¸ìš”.")

index_display = st.empty()
def update_index_info():
    while True:
        latest_index = get_latest_index()
        index_display.markdown(f"""
        **ğŸ“ˆ ìµœì‹  ì§€ìˆ˜ ì •ë³´:**
        - **S&P 500:** {latest_index['S&P 500']} 
        - **Nasdaq:** {latest_index['Nasdaq']} 
        - **Dow Jones:** {latest_index['Dow Jones']}
        - **KOSPI:** {latest_index['KOSPI']}
        - **KOSDAQ:** {latest_index['KOSDAQ']}
        """, unsafe_allow_html=True)
        time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì—…ë°ì´íŠ¸

# ì§€ìˆ˜ ì •ë³´ ì—…ë°ì´íŠ¸ ìŠ¤ë ˆë“œ ì‹œì‘
import threading
threading.Thread(target=update_index_info, daemon=True).start()

# ì‚¬ì´ë“œë°”ì— ê²€ìƒ‰ ì¡°ê±´ ë°°ì¹˜
with st.sidebar:
    st.header("ê²€ìƒ‰ ì¡°ê±´")
    source = st.radio("ê²€ìƒ‰í•  ì†ŒìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:", ("YouTube", "ë‰´ìŠ¤"))
    keyword1 = st.text_input("ì²« ë²ˆì§¸ í‚¤ì›Œë“œ", key="keyword1")
    keyword2 = st.text_input("ë‘ ë²ˆì§¸ í‚¤ì›Œë“œ (ì„ íƒ ì‚¬í•­)", key="keyword2")
    keyword3 = st.text_input("ì„¸ ë²ˆì§¸ í‚¤ì›Œë“œ (ì„ íƒ ì‚¬í•­)", key="keyword3")

    period = st.selectbox("ì¡°íšŒ ê¸°ê°„", ["ëª¨ë‘", "ìµœê·¼ 1ì¼", "ìµœê·¼ 1ì£¼ì¼", "ìµœê·¼ 1ê°œì›”", "ìµœê·¼ 3ê°œì›”", "ìµœê·¼ 6ê°œì›”", "ìµœê·¼ 1ë…„"], index=2)

    search_button = st.button("ê²€ìƒ‰ ì‹¤í–‰")

# ê²€ìƒ‰ ê²°ê³¼ ì €ì¥ìš© ì„¸ì…˜ ìƒíƒœ
if 'search_results' not in st.session_state:
    st.session_state.search_results = {'videos': [], 'news': []}
    st.session_state.total_results = 0

# ìš”ì•½ ê²°ê³¼ ì €ì¥ìš© ì„¸ì…˜ ìƒíƒœ
if 'summary' not in st.session_state:
    st.session_state.summary = ""

# ê²€ìƒ‰ ì‹¤í–‰
if search_button:
    keywords = " ".join(filter(None, [keyword1, keyword2, keyword3]))
    if keywords:
        with st.spinner(f"{source}ë¥¼ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            published_after = get_published_after(period)
            
            if source == "YouTube":
                # YouTube ì˜ìƒ ê²€ìƒ‰
                videos, total_video_results = search_videos_with_transcript(keywords, published_after)
                st.session_state.search_results = {'videos': videos, 'news': []}
                st.session_state.total_results = total_video_results
            
            elif source == "ë‰´ìŠ¤":
                # ë‰´ìŠ¤ ê²€ìƒ‰
                news_articles = search_news(keywords, published_after, max_results=10)
                total_news_results = len(news_articles)
                st.session_state.search_results = {'videos': [], 'news': news_articles}
                st.session_state.total_results = total_news_results
            
            # ê²€ìƒ‰ ì‹¤í–‰ ì‹œ ìš”ì•½ ê²°ê³¼ ì´ˆê¸°í™”
            st.session_state.summary = ""
            if not st.session_state.total_results:
                st.warning(f"{source}ì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")
    else:
        st.warning("í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

# ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
if source == "YouTube":
    st.subheader(f"ê²€ìƒ‰ëœ ì´ YouTube ì˜ìƒ: {st.session_state.total_results}ê°œ")
    for video in st.session_state.search_results['videos']:
        col1, col2 = st.columns([1, 2])
        with col1:
            st.image(video['snippet']['thumbnails']['medium']['url'], use_column_width=True)
        with col2:
            st.subheader(video['snippet']['title'])
            st.markdown(f"**ì±„ë„ëª…:** {video['snippet']['channelTitle']}")
            st.write(video['snippet']['description'])
            video_url = f"https://www.youtube.com/watch?v={video['id']['videoId']}"
            st.markdown(f"[ì˜ìƒ ë³´ê¸°]({video_url})")
            
            video_id = video['id']['videoId']
            video_title = video['snippet']['title']
            if st.button(f"ìš”ì•½ ë³´ê³ ì„œ ìš”ì²­ (ê²°ê³¼ëŠ” í™”ë©´ í•˜ë‹¨ì—ì„œ í™•ì¸í•˜ì„¸ìš”.)", key=f"summarize_{video_id}"):
                with st.spinner("ì˜ìƒì„ ìš”ì•½í•˜ëŠ” ì¤‘..."):
                    summary = summarize_video(video_id, video_title)
                    st.session_state.summary = summary
        st.divider()

elif source == "ë‰´ìŠ¤":
    st.subheader(f"ê²€ìƒ‰ëœ ì´ ë‰´ìŠ¤ ê¸°ì‚¬: {st.session_state.total_results}ê°œ")
    for i, article in enumerate(st.session_state.search_results['news']):
        st.subheader(article['title'])
        st.markdown(f"**ì¶œì²˜:** {article['source']['name']}")
        st.write(article['description'])
        st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({article['url']})")
        
        if st.button(f"ìš”ì•½ ë³´ê³ ì„œ ìš”ì²­ (ê²°ê³¼ëŠ” í™”ë©´ í•˜ë‹¨ì—ì„œ í™•ì¸í•˜ì„¸ìš”.)", key=f"summarize_news_{i}"):
            with st.spinner("ê¸°ì‚¬ë¥¼ ìš”ì•½í•˜ëŠ” ì¤‘..."):
                summary = summarize_news_article(article)
                st.session_state.summary = summary
        
        st.divider()

# ìš”ì•½ ê²°ê³¼ í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
st.markdown('<div class="fixed-footer">', unsafe_allow_html=True)
col1, col2 = st.columns([0.85, 0.15])  # ì—´ì„ ë¹„ìœ¨ë¡œ ë¶„í• 
with col1:
    st.subheader("ìš”ì•½ ë³´ê³ ì„œ")
with col2:
    if st.session_state.summary:
        download_summary_file(st.session_state.summary)

if st.session_state.summary:
    st.markdown(f'<div class="scrollable-container">{st.session_state.summary}</div>', unsafe_allow_html=True)
else:
    st.write("ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ìš”ì•½í•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”.")
st.markdown('</div>', unsafe_allow_html=True)

# ì£¼ì˜ì‚¬í•­ ë° ì•ˆë‚´
st.sidebar.markdown("---")
st.sidebar.markdown("**ì•ˆë‚´ì‚¬í•­:**")
st.sidebar.markdown("- ì´ ì„œë¹„ìŠ¤ëŠ” Google AI Studio API, YouTube Data API, Google News APIë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
st.sidebar.markdown("- ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆê³¼ ë³µì¡ë„ì— ë”°ë¼ ì²˜ë¦¬ ì‹œê°„ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
st.sidebar.markdown("- ì €ì‘ê¶Œ ë³´í˜¸ë¥¼ ìœ„í•´ ê°œì¸ì ì¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
